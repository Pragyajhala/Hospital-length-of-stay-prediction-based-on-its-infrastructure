{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Location  Time  Hospital_Stay  MRI_Units  CT_Scanners  Hospital_Beds\n",
      "0        AUS  1992            6.6       1.43        16.71           1.43\n",
      "1        AUS  1994            6.4       2.36        18.48           2.36\n",
      "2        AUS  1995            6.5       2.89        20.55           2.89\n",
      "3        AUS  1996            6.4       2.96        21.95           2.96\n",
      "4        AUS  1997            6.2       3.53        23.34           3.53\n",
      "..       ...   ...            ...        ...          ...            ...\n",
      "513      LTU  2014            6.8      10.57        22.17          10.57\n",
      "514      LTU  2015            6.6      11.02        21.00          11.02\n",
      "515      LTU  2016            6.6      12.20        23.01          12.20\n",
      "516      LTU  2017            6.5      12.37        23.33          12.37\n",
      "517      LTU  2018            6.5      12.49        24.27          12.49\n",
      "\n",
      "[518 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#reading csv file\n",
    "healthcare_dataset = pd.read_csv(\"D:/Sem 6 - Study Materials/Machine_Learning/SA/Healthcare_Investments_and_Hospital_Stay.csv\")\n",
    "print(healthcare_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(df, column):\n",
    "    df = df.copy()\n",
    "    dummies = pd.get_dummies(df[column])\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inputs(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # One-hot encode Location column\n",
    "    df = onehot_encode(df, column='Location')\n",
    "    #print(df)\n",
    "    # Split df into X and y\n",
    "    y = df['Hospital_Stay'].copy()\n",
    "    X = df.drop('Hospital_Stay', axis=1).copy()\n",
    "    \n",
    "    #splitting into train and test dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=123)\n",
    "    \n",
    "    #Scale X with a standard scaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    X_train = pd.DataFrame(scaler.transform(X_train), columns=X.columns)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_inputs(healthcare_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>MRI_Units</th>\n",
       "      <th>CT_Scanners</th>\n",
       "      <th>Hospital_Beds</th>\n",
       "      <th>AUS</th>\n",
       "      <th>AUT</th>\n",
       "      <th>BEL</th>\n",
       "      <th>CAN</th>\n",
       "      <th>CZE</th>\n",
       "      <th>DEU</th>\n",
       "      <th>...</th>\n",
       "      <th>LVA</th>\n",
       "      <th>NLD</th>\n",
       "      <th>NZL</th>\n",
       "      <th>POL</th>\n",
       "      <th>PRT</th>\n",
       "      <th>RUS</th>\n",
       "      <th>SVK</th>\n",
       "      <th>SVN</th>\n",
       "      <th>TUR</th>\n",
       "      <th>USA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.303643</td>\n",
       "      <td>0.502340</td>\n",
       "      <td>-0.349986</td>\n",
       "      <td>0.502340</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.215041</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.16855</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.159674</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.140422</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.140422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.738679</td>\n",
       "      <td>-0.697320</td>\n",
       "      <td>-0.873325</td>\n",
       "      <td>-0.697320</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.215041</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.16855</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.159674</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.140422</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.140422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.131392</td>\n",
       "      <td>-0.562147</td>\n",
       "      <td>-0.392384</td>\n",
       "      <td>-0.562147</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.215041</td>\n",
       "      <td>4.809712</td>\n",
       "      <td>-0.16855</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.159674</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.140422</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.140422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.276404</td>\n",
       "      <td>-0.018076</td>\n",
       "      <td>-0.295665</td>\n",
       "      <td>-0.018076</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.215041</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.16855</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>6.262765</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.140422</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.140422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.001462</td>\n",
       "      <td>1.174825</td>\n",
       "      <td>1.272365</td>\n",
       "      <td>1.174825</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.215041</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.16855</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.159674</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.140422</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.140422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>1.146474</td>\n",
       "      <td>-0.248996</td>\n",
       "      <td>-0.302952</td>\n",
       "      <td>-0.248996</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.215041</td>\n",
       "      <td>4.809712</td>\n",
       "      <td>-0.16855</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.159674</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.140422</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.140422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>0.276404</td>\n",
       "      <td>-0.678170</td>\n",
       "      <td>-0.378472</td>\n",
       "      <td>-0.678170</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.215041</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.16855</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.159674</td>\n",
       "      <td>5.648813</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.140422</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.140422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>-0.013620</td>\n",
       "      <td>-0.589181</td>\n",
       "      <td>-0.850140</td>\n",
       "      <td>-0.589181</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.215041</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.16855</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.159674</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.140422</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.140422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>-0.013620</td>\n",
       "      <td>-0.317709</td>\n",
       "      <td>-0.623580</td>\n",
       "      <td>-0.317709</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.215041</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.16855</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.159674</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.140422</td>\n",
       "      <td>5.181327</td>\n",
       "      <td>-0.140422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>0.421415</td>\n",
       "      <td>-0.539618</td>\n",
       "      <td>0.003102</td>\n",
       "      <td>-0.539618</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.215041</td>\n",
       "      <td>-0.207913</td>\n",
       "      <td>-0.16855</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.159674</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.177028</td>\n",
       "      <td>-0.140422</td>\n",
       "      <td>-0.193001</td>\n",
       "      <td>-0.140422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>362 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Time  MRI_Units  CT_Scanners  Hospital_Beds       AUS       AUT  \\\n",
       "0   -0.303643   0.502340    -0.349986       0.502340 -0.207913 -0.207913   \n",
       "1   -0.738679  -0.697320    -0.873325      -0.697320 -0.207913 -0.207913   \n",
       "2    0.131392  -0.562147    -0.392384      -0.562147 -0.207913 -0.207913   \n",
       "3    0.276404  -0.018076    -0.295665      -0.018076 -0.207913 -0.207913   \n",
       "4    1.001462   1.174825     1.272365       1.174825 -0.207913 -0.207913   \n",
       "..        ...        ...          ...            ...       ...       ...   \n",
       "357  1.146474  -0.248996    -0.302952      -0.248996 -0.207913 -0.207913   \n",
       "358  0.276404  -0.678170    -0.378472      -0.678170 -0.207913 -0.207913   \n",
       "359 -0.013620  -0.589181    -0.850140      -0.589181 -0.207913 -0.207913   \n",
       "360 -0.013620  -0.317709    -0.623580      -0.317709 -0.207913 -0.207913   \n",
       "361  0.421415  -0.539618     0.003102      -0.539618 -0.207913 -0.207913   \n",
       "\n",
       "          BEL       CAN       CZE      DEU  ...       LVA       NLD       NZL  \\\n",
       "0   -0.177028 -0.215041 -0.207913 -0.16855  ... -0.193001 -0.177028 -0.159674   \n",
       "1   -0.177028 -0.215041 -0.207913 -0.16855  ... -0.193001 -0.177028 -0.159674   \n",
       "2   -0.177028 -0.215041  4.809712 -0.16855  ... -0.193001 -0.177028 -0.159674   \n",
       "3   -0.177028 -0.215041 -0.207913 -0.16855  ... -0.193001 -0.177028  6.262765   \n",
       "4   -0.177028 -0.215041 -0.207913 -0.16855  ... -0.193001 -0.177028 -0.159674   \n",
       "..        ...       ...       ...      ...  ...       ...       ...       ...   \n",
       "357 -0.177028 -0.215041  4.809712 -0.16855  ... -0.193001 -0.177028 -0.159674   \n",
       "358 -0.177028 -0.215041 -0.207913 -0.16855  ... -0.193001 -0.177028 -0.159674   \n",
       "359 -0.177028 -0.215041 -0.207913 -0.16855  ... -0.193001 -0.177028 -0.159674   \n",
       "360 -0.177028 -0.215041 -0.207913 -0.16855  ... -0.193001 -0.177028 -0.159674   \n",
       "361 -0.177028 -0.215041 -0.207913 -0.16855  ... -0.193001 -0.177028 -0.159674   \n",
       "\n",
       "          POL       PRT       RUS       SVK       SVN       TUR       USA  \n",
       "0   -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
       "1   -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
       "2   -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
       "3   -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
       "4   -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "357 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
       "358  5.648813 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
       "359 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
       "360 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422  5.181327 -0.140422  \n",
       "361 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
       "\n",
       "[362 rows x 36 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121     7.2\n",
       "378     7.3\n",
       "91      6.7\n",
       "310     6.1\n",
       "479     5.9\n",
       "       ... \n",
       "329     6.6\n",
       "385     6.0\n",
       "79      7.5\n",
       "172     5.6\n",
       "439    14.0\n",
       "Name: Hospital_Stay, Length: 518, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = y_train.append(y_test)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_new = Y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_whole = pd.concat([X_train, X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_whole.insert(36, \"Hospital Stay\", Y_new, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframes with limited features\n",
    "X_lim = pd.DataFrame(X_whole[['Time', 'CT_Scanners', 'AUS', 'DEU', 'DNK', 'FRA', 'ISR', 'JPN', 'KOR', 'RUS', 'TUR', 'USA']], columns =['Time', 'CT_Scanners', 'AUS', 'DEU', 'DNK', 'FRA', 'ISR', 'JPN', 'KOR', 'RUS', 'TUR', 'USA'])\n",
    "Y_lim = X_whole['Hospital Stay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_lim, Y_lim, test_size = 0.3, random_state=123)  #splitting data into train and test set\n",
    "trindex=list(X_train.index)\n",
    "teindex=list(X_test.index)\n",
    "\n",
    "#converting into array\n",
    "X_train=np.array(X_train)\n",
    "Y_train=np.array(Y_train)\n",
    "X_test=np.array(X_test)\n",
    "Y_test=np.array(Y_test)\n",
    "X_test1=X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "shape of X_train is:  (362, 12)\n",
      "shape of Y_train is:  (362,)\n",
      "shape of X_test is:  (156, 12)\n",
      "shape of Y_test is:  (156,)\n"
     ]
    }
   ],
   "source": [
    "#printing shape of array\n",
    "print(\"\\nshape of X_train is: \",X_train.shape)\n",
    "print(\"shape of Y_train is: \",Y_train.shape)\n",
    "print(\"shape of X_test is: \",X_test.shape)\n",
    "print(\"shape of Y_test is: \",Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy=(np.ones([362,1],dtype=int)) \n",
    "\n",
    "# concatenating 1's for intercept calculation\n",
    "X_train=np.concatenate((dummy,X_train),axis=1) \n",
    "\n",
    "dummy_test=(np.ones([156,1],dtype=int))\n",
    "\n",
    "# concatenating 1's for intercept calculation\n",
    "X_test=np.concatenate((dummy_test,X_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Stochastic Gradient Descent\n",
      "2. Batch Gradient Descent\n",
      "\n",
      "Enter your choice 1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1. Stochastic Gradient Descent\")\n",
    "print(\"2. Batch Gradient Descent\")\n",
    "\n",
    "ch=eval(input(\"\\nEnter your choice \"))\n",
    "epochs=350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean-Squared-Error loss\n",
    "def calculate_error(theta):\n",
    "    error=0\n",
    "    for i in range(Y_train.shape[0]):\n",
    "        error+=np.square(np.dot(theta.transpose(),X_train[i])-Y_train[i])\n",
    "    mse=error/(2*Y_train.shape[0])\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#array with random values of coefficients\n",
    "theta=np.random.rand(13,1) \n",
    "\n",
    "# array for storing updated value of theta\n",
    "temp=np.ndarray([13,1]) \n",
    "\n",
    "#learning rate\n",
    "alpha=0.016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial value of mean square error was:  [24.09566772]\n",
      "reduced value of mean square error is:  [0.49771667]\n"
     ]
    }
   ],
   "source": [
    "#gradient descent calculation\n",
    "if ch==1:\n",
    "    y=[]\n",
    "    for epoch in range(epochs):\n",
    "        for j in range(13):\n",
    "            dif=0\n",
    "            for i in range(Y_train.shape[0]):\n",
    "                dif+=(np.dot(theta.transpose(),X_train[i].reshape([13,1])) - Y_train[i])*X_train[i][j]\n",
    "            temp[j] = theta[j] - (alpha/362)*(dif)\n",
    "            theta=temp   \n",
    "        y.append(calculate_error(theta))\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    x=np.arange(0,epochs)\n",
    "    plt.plot(x,y,color='b')\n",
    "    plt.show()\n",
    "if ch==2:\n",
    "    y=[]\n",
    "    for epoch in range(epochs):\n",
    "        for j in range(13):\n",
    "            dif=0\n",
    "            for i in range(Y_train.shape[0]):\n",
    "                dif+=(np.dot(theta.transpose(),X_train[i].reshape([13,1])) - Y_train[i])*X_train[i][j]\n",
    "            temp[j] = theta[j] - (alpha/362)*(dif)\n",
    "        theta=temp\n",
    "        y.append(calculate_error(theta))\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    x=np.arange(0,epochs)\n",
    "    plt.plot(x,y,color='b')\n",
    "    plt.show()\n",
    "print(\"initial value of mean square error was: \",y[0])\n",
    "print(\"reduced value of mean square error is: \",y[(len(y)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average percentage error for train set is: 9.211339756817141% \n",
      "value of R2 for train set is:              0.8645049570103774\n",
      "RMSE value for train set is:               0.9977140581121202\n"
     ]
    }
   ],
   "source": [
    "#calculation of average percentage error in predicted LOS for train set\n",
    "per=[]\n",
    "for i in range(Y_train.shape[0]):\n",
    "    temp=((np.abs((np.dot(theta.T,X_train[i].reshape(13,1)))-(Y_train[i])))/(Y_train[i]))*100\n",
    "    per.append(temp)\n",
    "avgp=sum(per)/len(per)\n",
    "print(\"average percentage error for train set is: {}% \".format(avgp[0][0]))\n",
    "\n",
    "#calculation of R2 and root mean square error for checking performance of train set\n",
    "lis=[]\n",
    "for i in range(Y_train.shape[0]):\n",
    "    temp=np.sum(np.dot(theta.T,X_train[i].reshape(13,1)))\n",
    "    lis.append(temp)\n",
    "Y_train_pred=np.array(lis)\n",
    "r=r2_score(Y_train,Y_train_pred)\n",
    "print(\"value of R2 for train set is:             \",r)\n",
    "rmse = (np.sqrt(mean_squared_error(Y_train,Y_train_pred)))\n",
    "print(\"RMSE value for train set is:              \",rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average percentage error for test set is: 9.263507009189572% \n",
      "value of R2 for test set is :             0.8131723205169401\n",
      "value of rmse for test set is :           0.9434278013972442\n"
     ]
    }
   ],
   "source": [
    "#calculation of average percentage error in predicted LOS for test set\n",
    "per1=[]\n",
    "for i in range(Y_test.shape[0]):\n",
    "    temp=((np.abs((np.dot(theta.T,X_test[i].reshape(13,1)))-(Y_test[i])))/(Y_test[i]))*100\n",
    "    per1.append(temp)\n",
    "avgp1=sum(per1)/len(per1)\n",
    "print(\"average percentage error for test set is: {}% \".format(avgp1[0][0]))\n",
    "\n",
    "#calculation of R2 and root mean square error for checking performance of test set\n",
    "lis1=[]\n",
    "for i in range(Y_test.shape[0]):\n",
    "    temp=np.sum(np.dot(theta.T,X_test[i].reshape(13,1)))\n",
    "    lis1.append(temp)\n",
    "Y_test_pred=np.array(lis1)\n",
    "r1=r2_score(Y_test,Y_test_pred)\n",
    "print(\"value of R2 for test set is :            \",r1)\n",
    "rmse = (np.sqrt(mean_squared_error(Y_test,Y_test_pred)))\n",
    "print(\"value of rmse for test set is :          \",rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below data is in NORMALIZED FORM\n",
      "        Time  CT_Scanners       AUS      DEU       DNK       FRA       ISR  \\\n",
      "0   1.291486    -0.699762 -0.207913 -0.16855 -0.074536 -0.207913  5.181327   \n",
      "1   1.001462     0.060073 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "2   0.856451    -0.411595 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "3  -0.013620    -0.654053 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "4  -0.883690    -0.826291 -0.207913 -0.16855 -0.074536  4.809712 -0.193001   \n",
      "5  -0.303643    -0.839540 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "6   1.001462     0.875554 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "7  -0.448655    -0.118127 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "8  -0.448655    -0.858089 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "9  -2.043784    -1.233038 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "10  1.001462    -0.690488 -0.207913 -0.16855 -0.074536 -0.207913  5.181327   \n",
      "11  1.291486     0.959024 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "12  0.566427     0.242910 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "13  0.131392    -0.445380 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "14 -1.753761     0.123006  4.809712 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "15  1.291486     0.214425 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "16 -0.303643    -0.517587 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "17 -0.303643     0.609248 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "18  1.001462     2.613173  4.809712 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "19 -2.623831    -1.203228 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "20  0.421415     0.864293 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "21  1.436498     0.579437 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "22  1.001462    -0.772632 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "23 -0.448655    -0.901149 -0.207913 -0.16855 -0.074536 -0.207913  5.181327   \n",
      "24  0.856451     1.299525 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "25  1.436498    -0.265192 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "26  0.421415    -0.289703 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "27  0.856451     0.848394 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "28  0.566427    -0.823641 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "29  0.566427     0.882179 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "30  1.291486    -0.095604 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "31  0.421415    -0.352636 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "32  0.711439    -0.809067 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "33  0.421415    -0.491752 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "34  0.131392     0.613885 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "35 -0.303643    -0.764020 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "36 -1.173714    -0.435443 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "37 -1.173714    -0.962094 -0.207913 -0.16855 -0.074536 -0.207913  5.181327   \n",
      "38  1.146474    -0.467241 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "39  1.146474     1.172996 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "40  1.001462    -0.137339 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "41 -0.738679    -0.796481 -0.207913 -0.16855 -0.074536  4.809712 -0.193001   \n",
      "42  0.421415    -0.501689 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "43 -0.158632     0.941800 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "44  1.146474    -0.189010 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "45 -1.608749    -1.184679 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "46 -0.883690     0.301869 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "47  0.421415    -0.769320 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "48  0.421415    -0.228757 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "49 -0.013620     0.755650 -0.207913 -0.16855 -0.074536 -0.207913 -0.193001   \n",
      "\n",
      "         JPN       KOR       RUS       TUR       USA  \n",
      "0  -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "1  -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "2  -0.140422 -0.193001 -0.193001  5.181327 -0.140422  \n",
      "3  -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "4  -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "5  -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "6  -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "7  -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "8  -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "9  -0.140422 -0.193001  5.181327 -0.193001 -0.140422  \n",
      "10 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "11 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "12 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "13 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "14 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "15 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "16 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "17 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "18 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "19 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "20 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "21 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "22 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "23 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "24 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "25 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "26 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "27 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "28 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "29 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "30 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "31 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "32 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "33 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "34 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "35 -0.140422 -0.193001 -0.193001  5.181327 -0.140422  \n",
      "36 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "37 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "38 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "39 -0.140422  5.181327 -0.193001 -0.193001 -0.140422  \n",
      "40 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "41 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "42 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "43 -0.140422 -0.193001 -0.193001 -0.193001  7.121396  \n",
      "44 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "45 -0.140422 -0.193001  5.181327 -0.193001 -0.140422  \n",
      "46 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "47 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "48 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n",
      "49 -0.140422 -0.193001 -0.193001 -0.193001 -0.140422  \n"
     ]
    }
   ],
   "source": [
    "#Predicting LOS\n",
    "temp=X_test1[0:50]\n",
    "df1=pd.DataFrame(temp,columns=['Time', 'CT_Scanners', 'AUS', 'DEU', 'DNK', 'FRA', 'ISR', 'JPN', 'KOR', 'RUS', 'TUR', 'USA'])\n",
    "print(\"Below data is in NORMALIZED FORM\")\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the index no. from above data for which you want to predict LOS: 41\n",
      "\n",
      "PREDICTED LENGTH OF STAY IS:  6.223596549798638\n",
      "ORIGINAL LENGTH OF STAY :  6.1\n"
     ]
    }
   ],
   "source": [
    "ind=int(input(\"\\nEnter the index no. from above data for which you want to predict LOS: \"))\n",
    "LOS=np.dot(theta.transpose(),X_test[ind].reshape(13,1))\n",
    "\n",
    "print(\"\\nPREDICTED LENGTH OF STAY IS: \",LOS[0][0])\n",
    "print(\"ORIGINAL LENGTH OF STAY : \",Y_test[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
