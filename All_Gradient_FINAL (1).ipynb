{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Location  Time  Hospital_Stay  MRI_Units  CT_Scanners  Hospital_Beds\n",
      "0        AUS  1992            6.6       1.43        16.71           1.43\n",
      "1        AUS  1994            6.4       2.36        18.48           2.36\n",
      "2        AUS  1995            6.5       2.89        20.55           2.89\n",
      "3        AUS  1996            6.4       2.96        21.95           2.96\n",
      "4        AUS  1997            6.2       3.53        23.34           3.53\n",
      "..       ...   ...            ...        ...          ...            ...\n",
      "513      LTU  2014            6.8      10.57        22.17          10.57\n",
      "514      LTU  2015            6.6      11.02        21.00          11.02\n",
      "515      LTU  2016            6.6      12.20        23.01          12.20\n",
      "516      LTU  2017            6.5      12.37        23.33          12.37\n",
      "517      LTU  2018            6.5      12.49        24.27          12.49\n",
      "\n",
      "[518 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#reading csv file\n",
    "healthcare_dataset = pd.read_csv(\"D:/Sem 6 - Study Materials/Machine_Learning/SA/Healthcare_Investments_and_Hospital_Stay.csv\")\n",
    "print(healthcare_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(df, column):\n",
    "    df = df.copy()\n",
    "    dummies = pd.get_dummies(df[column])\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inputs(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # One-hot encode Location column\n",
    "    df = onehot_encode(df, column='Location')\n",
    "    #print(df)\n",
    "    # Split df into X and y\n",
    "    y = df['Hospital_Stay'].copy()\n",
    "    X = df.drop('Hospital_Stay', axis=1).copy()\n",
    "    \n",
    "    #normalizing data values\n",
    "    #X_new=(X-X.mean())/(X.max()-X.min())\n",
    "    #X_new.describe()\n",
    "    \n",
    "    #splitting into train and test dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=123)\n",
    "    \n",
    "    #Scale X with a standard scaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    X_train = pd.DataFrame(scaler.transform(X_train), columns=X.columns)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_inputs(healthcare_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trindex=list(X_train.index)\n",
    "teindex=list(X_test.index)\n",
    "\n",
    "# converting into array\n",
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)\n",
    "X_test=np.array(X_test)\n",
    "y_test=np.array(y_test)\n",
    "X_test1=X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train is:  (362, 36)\n",
      "Shape of Y_train is:  (362,)\n",
      "Shape of X_test is:  (156, 36)\n",
      "Shape of Y_test is:  (156,)\n"
     ]
    }
   ],
   "source": [
    "# printing shape of train set\n",
    "print(\"Shape of X_train is: \",X_train.shape) \n",
    "print(\"Shape of Y_train is: \",y_train.shape)\n",
    "print(\"Shape of X_test is: \",X_test.shape)\n",
    "print(\"Shape of Y_test is: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy=(np.ones([362,1],dtype=int))\n",
    "#concatenating 1's for intercept calculation\n",
    "X_train=np.concatenate((dummy,X_train),axis=1)\n",
    "dummy_test=(np.ones([156,1],dtype=int))\n",
    "X_test=np.concatenate((dummy_test,X_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Stochastic Gradient Descent\n",
      "2. Batch Gradient Descent\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1. Stochastic Gradient Descent\")\n",
    "print(\"2. Batch Gradient Descent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your choice from above 2\n"
     ]
    }
   ],
   "source": [
    "ch=eval(input(\"\\nEnter your choice from above \"))\n",
    "epochs= 550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean-Squared-Error loss\n",
    "def calculate_error(theta):\n",
    "    error=0\n",
    "    for i in range(y_train.shape[0]):\n",
    "        error+=np.square(np.dot(theta.transpose(),X_train[i])-y_train[i])\n",
    "    mse=error/(2*y_train.shape[0])\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficients of features\n",
    "theta=np.random.rand(37,1)\n",
    "\n",
    "# for storing updated value of theta\n",
    "temp=np.ndarray([37,1])\n",
    "\n",
    "# learning rate\n",
    "alpha=0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting value of mean square error is :  [28.49619843]\n",
      "reduced value of mean square error:  [0.41531718]\n"
     ]
    }
   ],
   "source": [
    "#gradient descent calculation\n",
    "if ch==1:\n",
    "    y=[]\n",
    "    for epoch in range(epochs):\n",
    "        for j in range(37):\n",
    "            dif=0\n",
    "            for i in range(y_train.shape[0]):\n",
    "                dif+=(np.dot(theta.transpose(),X_train[i].reshape([37,1])) - y_train[i])*X_train[i][j]\n",
    "            temp[j] = theta[j] - (alpha/362)*(dif)\n",
    "            theta=temp   \n",
    "        y.append(calculate_error(theta))\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    x=np.arange(0,epochs)\n",
    "    plt.plot(x,y,color='b')\n",
    "    plt.show()\n",
    "if ch==2:\n",
    "    y=[]\n",
    "    for epoch in range(epochs):\n",
    "        for j in range(37):\n",
    "            dif=0\n",
    "            for i in range(y_train.shape[0]):\n",
    "                dif+=(np.dot(theta.transpose(),X_train[i].reshape([37,1])) - y_train[i])*X_train[i][j]\n",
    "            temp[j] = theta[j] - (alpha/362)*(dif)\n",
    "        theta=temp\n",
    "        y.append(calculate_error(theta))\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    x=np.arange(0,epochs)\n",
    "    plt.plot(x,y,color='b')\n",
    "    plt.show()\n",
    "    \n",
    "print(\"starting value of mean square error is : \",y[0])\n",
    "print(\"reduced value of mean square error: \",y[(len(y)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average percentage error for train set is: 7.081173285397436% \n",
      "value of R2 for train set is:              0.8882295885542902\n",
      "value of rmse for train set is:            0.9113914431046776\n"
     ]
    }
   ],
   "source": [
    "#calculation of average percentage error in predicted price for train set\n",
    "per=[]\n",
    "for i in range(y_train.shape[0]):\n",
    "    temp=((np.abs((np.dot(theta.T,X_train[i].reshape(37,1)))-(y_train[i])))/(y_train[i]))*100\n",
    "    per.append(temp)\n",
    "avgp=sum(per)/len(per)\n",
    "print(\"average percentage error for train set is: {}% \".format(avgp[0][0]))\n",
    "\n",
    "#calculation of R2 and Root mean square for checking performance of train set on model\n",
    "lis=[]\n",
    "for i in range(y_train.shape[0]):\n",
    "    temp=np.dot(theta.T,X_train[i].reshape(37,1))\n",
    "    lis.append(temp[0][0])\n",
    "Y_train_pred=np.array(lis)\n",
    "\n",
    "r=r2_score(y_train,Y_train_pred)\n",
    "print(\"value of R2 for train set is:             \",r)\n",
    "rmse = (np.sqrt(mean_squared_error(y_train,Y_train_pred)))\n",
    "print(\"value of rmse for train set is:           \",rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average percentage error for test set is: 8.066320656981638% \n",
      "value of R2 for test set is:              0.8705849305713449\n",
      "value of rmse for test set is:            0.770699231622185\n"
     ]
    }
   ],
   "source": [
    "#calculation of average percentage error in predicted price for test set\n",
    "per1=[]\n",
    "for i in range(y_test.shape[0]):\n",
    "    temp=((np.abs((np.dot(theta.T,X_test[i].reshape(37,1)))-(y_test[i])))/(y_test[i]))*100\n",
    "    per1.append(temp)\n",
    "avgp1=sum(per1)/len(per1)\n",
    "print(\"average percentage error for test set is: {}% \".format(avgp1[0][0]))\n",
    "\n",
    "#calculatiom of R2 and Root mean square error for checking performance of test set on model\n",
    "\n",
    "lis1=[]\n",
    "for i in range(y_test.shape[0]):\n",
    "    temp=np.dot(theta.T,X_test[i].reshape(37,1))\n",
    "    lis1.append(temp[0][0])\n",
    "Y_test_pred=np.array(lis1)\n",
    "\n",
    "r1=r2_score(y_test,Y_test_pred)\n",
    "print(\"value of R2 for test set is:             \",r1)\n",
    "rmse = (np.sqrt(mean_squared_error(y_test,Y_test_pred)))\n",
    "print(\"value of rmse for test set is:           \",rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below data is in NORMALIZED FORM\n",
      "        Time  MRI_Units  CT_Scanners  Hospital_Beds       AUS       AUT  \\\n",
      "0  -0.158632  -0.591434    -0.647429      -0.591434 -0.207913 -0.207913   \n",
      "1   0.856451  -0.465273    -0.295665      -0.465273 -0.207913 -0.207913   \n",
      "2  -2.623831  -1.110724    -0.848152      -1.110724 -0.207913 -0.207913   \n",
      "3   0.566427  -0.101433    -0.179073      -0.101433 -0.207913 -0.207913   \n",
      "4  -0.883690   0.763674     0.461520       0.763674 -0.207913 -0.207913   \n",
      "5  -1.318726   0.277052     0.331017       0.277052 -0.207913 -0.207913   \n",
      "6   0.131392   0.870686     0.613885       0.870686 -0.207913  4.809712   \n",
      "7  -1.463737  -0.700699     0.270733      -0.700699  4.809712 -0.207913   \n",
      "8  -2.333808  -1.176057    -1.126383      -1.176057 -0.207913 -0.207913   \n",
      "9  -1.608749   0.076545     0.265434       0.076545 -0.207913 -0.207913   \n",
      "10  0.566427  -0.500193    -0.302290      -0.500193 -0.207913 -0.207913   \n",
      "11  0.711439   0.242132     0.131618       0.242132 -0.207913 -0.207913   \n",
      "12  0.131392  -0.317709    -0.416894      -0.317709 -0.207913 -0.207913   \n",
      "13  1.146474  -0.322215    -0.183048      -0.322215 -0.207913 -0.207913   \n",
      "14  0.421415   1.187216     1.039843       1.187216 -0.207913 -0.207913   \n",
      "15 -2.333808  -1.008217    -0.851464      -1.008217 -0.207913 -0.207913   \n",
      "16 -0.158632   0.787329     0.657607       0.787329 -0.207913  4.809712   \n",
      "17  1.001462   1.750436     1.121987       1.750436 -0.207913 -0.207913   \n",
      "18  1.436498   1.027261     1.861949       1.027261 -0.207913 -0.207913   \n",
      "19 -1.608749  -1.112976    -1.184679      -1.112976 -0.207913 -0.207913   \n",
      "20  0.276404  -0.470905    -0.806418      -0.470905 -0.207913 -0.207913   \n",
      "21  0.421415   1.316756     0.864293       1.316756 -0.207913 -0.207913   \n",
      "22  1.146474   0.317603    -0.208221       0.317603 -0.207913 -0.207913   \n",
      "23 -0.448655  -0.726607    -0.579196      -0.726607 -0.207913 -0.207913   \n",
      "24 -0.593667  -0.785182     1.693023      -0.785182  4.809712 -0.207913   \n",
      "25 -0.303643   1.200733     0.597986       1.200733 -0.207913 -0.207913   \n",
      "26 -2.043784  -1.121988    -1.233038      -1.121988 -0.207913 -0.207913   \n",
      "27 -0.303643  -0.790814    -0.839540      -0.790814 -0.207913 -0.207913   \n",
      "28  1.001462  -0.171272    -0.464591      -0.171272 -0.207913 -0.207913   \n",
      "29 -0.303643  -1.001459    -0.908436      -1.001459 -0.207913 -0.207913   \n",
      "30 -0.013620   0.174545     0.431047       0.174545 -0.207913 -0.207913   \n",
      "31  1.146474   2.929820     1.443278       2.929820 -0.207913 -0.207913   \n",
      "32  0.276404   0.167787    -0.513613       0.167787 -0.207913 -0.207913   \n",
      "33  0.421415   1.070066     0.082596       1.070066 -0.207913 -0.207913   \n",
      "34  0.421415  -0.132973    -0.461941      -0.132973 -0.207913 -0.207913   \n",
      "35  1.146474   0.103580     0.253510       0.103580 -0.207913 -0.207913   \n",
      "36  1.291486   1.838299     0.292594       1.838299 -0.207913 -0.207913   \n",
      "37  0.421415  -0.435985    -0.352636      -0.435985 -0.207913 -0.207913   \n",
      "38  0.276404   1.322389     0.800035       1.322389 -0.207913 -0.207913   \n",
      "39 -0.738679   0.038246     0.429060       0.038246 -0.207913 -0.207913   \n",
      "40  0.856451   1.741425     0.848394       1.741425 -0.207913 -0.207913   \n",
      "41 -1.318726  -1.109597    -1.172755      -1.109597 -0.207913 -0.207913   \n",
      "42 -0.448655  -0.423595    -0.483802      -0.423595 -0.207913 -0.207913   \n",
      "43  0.566427   0.122729    -0.607681       0.122729 -0.207913 -0.207913   \n",
      "44 -0.303643   0.684823     0.647670       0.684823 -0.207913  4.809712   \n",
      "45 -0.303643  -0.575664    -0.832253      -0.575664 -0.207913 -0.207913   \n",
      "46 -0.013620  -0.037225    -0.654053      -0.037225 -0.207913 -0.207913   \n",
      "47 -0.883690  -0.787435     0.945775      -0.787435  4.809712 -0.207913   \n",
      "48  1.001462   0.206086    -0.233394       0.206086 -0.207913 -0.207913   \n",
      "49 -0.593667  -0.510331    -0.859414      -0.510331 -0.207913 -0.207913   \n",
      "\n",
      "         BEL       CAN       CZE       DEU  ...       LVA       NLD       NZL  \\\n",
      "0  -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "1  -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "2  -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001  5.648813 -0.159674   \n",
      "3  -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "4  -0.177028 -0.215041 -0.207913  5.932959  ... -0.193001 -0.177028 -0.159674   \n",
      "5  -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "6  -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "7  -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "8  -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "9  -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "10 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "11 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "12 -0.177028  4.650269 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "13 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "14 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "15 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001  5.648813 -0.159674   \n",
      "16 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "17 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "18 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "19 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "20 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "21 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "22 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "23 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "24 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "25 -0.177028 -0.215041 -0.207913  5.932959  ... -0.193001 -0.177028 -0.159674   \n",
      "26 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "27 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "28 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "29 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "30 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "31 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "32 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001  5.648813 -0.159674   \n",
      "33 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "34 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "35  5.648813 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "36 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "37 -0.177028 -0.215041  4.809712 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "38 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "39 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "40 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "41 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "42  5.648813 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "43 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001  5.648813 -0.159674   \n",
      "44 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "45 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "46 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001  5.648813 -0.159674   \n",
      "47 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "48 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001 -0.177028 -0.159674   \n",
      "49 -0.177028 -0.215041 -0.207913 -0.168550  ... -0.193001  5.648813 -0.159674   \n",
      "\n",
      "         POL       PRT       RUS       SVK       SVN       TUR       USA  \n",
      "0  -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "1   5.648813 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "2  -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "3  -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "4  -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "5  -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001  7.121396  \n",
      "6  -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "7  -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "8  -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "9  -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001  7.121396  \n",
      "10 -0.177028 -0.052632 -0.193001  5.648813 -0.140422 -0.193001 -0.140422  \n",
      "11 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "12 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "13  5.648813 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "14 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "15 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "16 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "17 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "18 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "19 -0.177028 -0.052632  5.181327 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "20 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "21 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "22 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "23 -0.177028 -0.052632 -0.193001  5.648813 -0.140422 -0.193001 -0.140422  \n",
      "24 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "25 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "26 -0.177028 -0.052632  5.181327 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "27 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "28 -0.177028 -0.052632 -0.193001 -0.177028  7.121396 -0.193001 -0.140422  \n",
      "29 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "30 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "31 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001  7.121396  \n",
      "32 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "33 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "34 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422  5.181327 -0.140422  \n",
      "35 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "36 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "37 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "38 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "39 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "40 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "41 -0.177028 -0.052632  5.181327 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "42 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "43 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "44 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "45 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "46 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "47 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "48 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "49 -0.177028 -0.052632 -0.193001 -0.177028 -0.140422 -0.193001 -0.140422  \n",
      "\n",
      "[50 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "# Predicting values\n",
    "temp=X_test1[0:50]\n",
    "df1=pd.DataFrame(temp, columns=['Time','MRI_Units','CT_Scanners','Hospital_Beds','AUS','AUT','BEL','CAN','CZE','DEU','DNK', 'ESP', 'EST', 'FIN', 'FRA', 'GBR', 'GRC', 'HUN', 'IRL', 'ISL', 'ISR', 'ITA', 'JPN', 'KOR', 'LTU', 'LUX', 'LVA', 'NLD', 'NZL', 'POL', 'PRT', 'RUS', 'SVK', 'SVN', 'TUR', 'USA'])\n",
    "print(\"Below data is in NORMALIZED FORM\")\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the index no from above data for which you want to predict LOS: 14\n",
      "\n",
      "PREDICTED LENGTH OF STAY IS:  [9.03370019]\n",
      "ORIGINAL LENGTH OF STAY IS :  10.1\n"
     ]
    }
   ],
   "source": [
    "ind=int(input(\"\\nEnter the index no from above data for which you want to predict LOS: \"))\n",
    "LOS=np.dot(theta.transpose(),X_test[ind].reshape(37,1))\n",
    "\n",
    "print(\"\\nPREDICTED LENGTH OF STAY IS: \",LOS[0])\n",
    "print(\"ORIGINAL LENGTH OF STAY IS : \",y_test[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
